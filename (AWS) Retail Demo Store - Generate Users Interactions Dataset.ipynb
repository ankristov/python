{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d880f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dfb9b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date1_str: 2022-09-09\n",
      "date1_date: 2022-09-09 00:00:00\n",
      "date2_date: 2022-12-08 00:00:00\n",
      "date1_timestamp: 1662674400.0\n",
      "date1_timestamp_to_date: 2022-09-08 22:00:00\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "date1_str = '2022-09-09'\n",
    "date1_date = datetime.strptime(date1_str,'%Y-%m-%d')\n",
    "date2_date = date1_date + timedelta(90)\n",
    "date1_timestamp = date1_date.timestamp()\n",
    "date1_timestamp_to_date = datetime.utcfromtimestamp(date1_timestamp)\n",
    "print(f'date1_str: {date1_str}')\n",
    "print(f'date1_date: {date1_date}')\n",
    "print(f'date2_date: {date2_date}')\n",
    "print(f'date1_timestamp: {date1_timestamp}')\n",
    "print(f'date1_timestamp_to_date: {date1_timestamp_to_date}')\n",
    "print(type(date1_timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c825cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1654034400.0\n",
      "1661896800.0\n"
     ]
    }
   ],
   "source": [
    "FIRST_TIMESTAMP = datetime.strptime('2022-06-01','%Y-%m-%d').timestamp()\n",
    "LAST_TIMESTAMP = datetime.strptime('2022-08-31','%Y-%m-%d').timestamp()\n",
    "print(FIRST_TIMESTAMP)\n",
    "print(LAST_TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61b12d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average product price: $117.00\n",
      "Minimum interactions to generate: 675000\n",
      "Starting timestamp: 1654034400.0 (2022-06-01 00:00:00)\n",
      "Seconds increment: 11\n",
      "Generating interactions... (this may take a few minutes)\n",
      "Writing interactions to: /Users/Andrew/Desktop/(AWS) Retail Demo Store/interactions.csv\n",
      "Generated 19792 interactions so far (about 496 seconds to go)\n",
      "Generated 66454 interactions so far (about 412 seconds to go)\n",
      "Generated 116940 interactions so far (about 357 seconds to go)\n",
      "Generated 167357 interactions so far (about 318 seconds to go)\n",
      "Generated 217550 interactions so far (about 283 seconds to go)\n",
      "Generated 266025 interactions so far (about 253 seconds to go)\n",
      "Generated 303864 interactions so far (about 238 seconds to go)\n",
      "Generated 351668 interactions so far (about 206 seconds to go)\n",
      "Generated 399998 interactions so far (about 175 seconds to go)\n",
      "Generated 443265 interactions so far (about 148 seconds to go)\n",
      "Generated 494566 interactions so far (about 114 seconds to go)\n",
      "Generated 543649 interactions so far (about 83 seconds to go)\n",
      "Generated 594110 interactions so far (about 51 seconds to go)\n",
      "Generated 644926 interactions so far (about 18 seconds to go)\n",
      "Interactions generation done.\n",
      "Total interactions: 675004\n",
      "Total product viewed: 581900 (217473)\n",
      "Total product added: 46552 (17528)\n",
      "Total cart viewed: 29095 (10868)\n",
      "Total checkout started: 11638 (4359)\n",
      "Total order completed: 5819 (2207)\n",
      "Generation script finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script exists so that when developing or internal deployment of public commits\n",
    "the new Personalize training files can be generated, picked up, and uploaded.\n",
    "This script generates interactions for Amazon Personalize by heuristic simulation. It is based off the notebook\n",
    "under workshop/01-Personalization where the logic is explained in more detail.\n",
    "However, it has been improved in the following ways:\n",
    " 1. This script is deterministic; random seeds from RANDOM_SEED random variable below.\n",
    " 2. Logic exists for ensuring balance across categories.\n",
    " 3. Logic exists for ensuring balance across products.\n",
    " 4. Discount events are also generated according to 3 different types of users: discount-likers discount-indifferent,\n",
    "    and price-sensitive-discount-likers.\n",
    "Item 1 allows us to re-generate data during staging and item 2 and 3 helps recommendations look appropriate in\n",
    "the final demo. If there is poor balance across products and categories then one may not get recommendations\n",
    "for products in the same category. This is a hotfix for the logic whereby we generate profiles and probabilistically\n",
    "sample product categories according to the sample user profile. Item 4 is necessary for training the discounts\n",
    "personalizeation campaign.\n",
    "\"\"\"\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import random\n",
    "import yaml\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "# Keep things deterministic\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "# Where to put the generated data so that it is picked up by stage.sh\n",
    "#GENERATED_DATA_ROOT = \"src/aws-lambda/personalize-pre-create-resources/data\"\n",
    "GENERATED_DATA_ROOT = \"/Users/Andrew/Desktop/(AWS) Retail Demo Store\"\n",
    "\n",
    "# Interactions will be generated between these dates\n",
    "#FIRST_TIMESTAMP = 1591803782  # 2020-06-10, 18:43:02\n",
    "#LAST_TIMESTAMP = 1599579782  # 2020-09-08, 18:43:02\n",
    "FIRST_TIMESTAMP = datetime.strptime('2022-06-01','%Y-%m-%d').timestamp()\n",
    "LAST_TIMESTAMP = datetime.strptime('2022-08-31','%Y-%m-%d').timestamp()\n",
    "\n",
    "# Users are set up with 3 product categories on their personas. If [0.6, 0.25, 0.15] it means\n",
    "# 60% of the time they'll choose a product from the first category, etc.\n",
    "CATEGORY_AFFINITY_PROBS = [0.6, 0.25, 0.15]\n",
    "\n",
    "# After a product, there are this many products within the category that a user is likely to jump on next.\n",
    "# The purpose of this is to keep recommendations focused within the category if there are too many products\n",
    "# in a category, because at present the user profiles approach samples products from a category.\n",
    "PRODUCT_AFFINITY_N = 4\n",
    "\n",
    "# from 0 to 1. If 0 then products in busy categories get represented less. If 1 then all products same amount.\n",
    "NORMALISE_PER_PRODUCT_WEIGHT = 1.0\n",
    "\n",
    "# With this probability a product interaction will be with the product discounted\n",
    "# Here we go the other way - what is the probability that a product that a user is already interacting\n",
    "# with is discounted - depending on whether user likes discounts or not\n",
    "DISCOUNT_PROBABILITY = 0.2\n",
    "DISCOUNT_PROBABILITY_WITH_PREFERENCE = 0.5\n",
    "\n",
    "#IN_PRODUCTS_FILENAME = \"src/products/src/products-service/data/products.yaml\"\n",
    "#IN_USERS_FILENAMES = [\"src/users/src/users-service/data/users.json.gz\",\n",
    "#                      \"src/users/src/users-service/data/cstore_users.json.gz\"]\n",
    "IN_PRODUCTS_FILENAME = \"/Users/Andrew/Desktop/(AWS) Retail Demo Store/products.yaml\"\n",
    "IN_USERS_FILENAMES = [\"/Users/Andrew/Desktop/(AWS) Retail Demo Store/users.json.gz\", \n",
    "                      \"/Users/Andrew/Desktop/(AWS) Retail Demo Store/cstore_users.json.gz\"]\n",
    "\n",
    "\n",
    "PROGRESS_MONITOR_SECONDS_UPDATE = 30\n",
    "\n",
    "GENDER_ANY = 'Any'\n",
    "\n",
    "# This is where stage.sh will pick them up from\n",
    "out_items_filename = f\"{GENERATED_DATA_ROOT}/items.csv\"\n",
    "out_users_filename = f\"{GENERATED_DATA_ROOT}/users.csv\"\n",
    "out_interactions_filename = f\"{GENERATED_DATA_ROOT}/interactions.csv\"\n",
    "\n",
    "# The meaning of the below constants is described in the relevant notebook.\n",
    "\n",
    "# Minimum number of interactions to generate\n",
    "min_interactions = 675000\n",
    "# min_interactions = 50000\n",
    "\n",
    "# Percentages of each event type to generate\n",
    "product_added_percent = .08\n",
    "cart_viewed_percent = .05\n",
    "checkout_started_percent = .02\n",
    "order_completed_percent = .01\n",
    "\n",
    "def generate_user_items(out_users_filename, out_items_filename, in_users_filenames, in_products_filename):\n",
    "\n",
    "    Path(out_items_filename).parents[0].mkdir(parents=True, exist_ok=True)\n",
    "    Path(out_users_filename).parents[0].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Product info is stored in the repository\n",
    "    with open(in_products_filename, 'r') as f:\n",
    "        products = yaml.safe_load(f)\n",
    "\n",
    "    products_df = pd.DataFrame(products)\n",
    "\n",
    "    # User info is stored in the repository - it was automatically generated\n",
    "    users = []\n",
    "    for in_users_filename in in_users_filenames:\n",
    "        with gzip.open(in_users_filename, 'r') as f:\n",
    "            users += json.load(f)\n",
    "\n",
    "    users_df = pd.DataFrame(users)\n",
    "\n",
    "    products_dataset_df = products_df[['id','price','category','style','description','gender_affinity']]\n",
    "    products_dataset_df = products_dataset_df.rename(columns = {'id':'ITEM_ID',\n",
    "                                                            'price':'PRICE',\n",
    "                                                            'category':'CATEGORY_L1',\n",
    "                                                            'style':'CATEGORY_L2',\n",
    "                                                            'description':'PRODUCT_DESCRIPTION',\n",
    "                                                            'gender_affinity':'GENDER'})\n",
    "    # Since GENDER column requires a value for all rows, default all nulls to \"Any\"\n",
    "    products_dataset_df['GENDER'].fillna(GENDER_ANY, inplace = True)\n",
    "    products_dataset_df.to_csv(out_items_filename, index=False)\n",
    "\n",
    "    users_dataset_df = users_df[['id', 'age', 'gender']]\n",
    "    users_dataset_df = users_dataset_df.rename(columns={'id': 'USER_ID',\n",
    "                                                        'age': 'AGE',\n",
    "                                                        'gender': 'GENDER'})\n",
    "\n",
    "    users_dataset_df.to_csv(out_users_filename, index=False)\n",
    "\n",
    "    return users_df, products_df\n",
    "\n",
    "def generate_interactions(out_interactions_filename, users_df, products_df):\n",
    "    \"\"\"Generate items.csv, users.csv from users and product dataframes makes interactions.csv by simulating some\n",
    "    shopping behaviour.\"\"\"\n",
    "\n",
    "    # Count of interactions generated for each event type\n",
    "    product_viewed_count = 0\n",
    "    discounted_product_viewed_count = 0\n",
    "    product_added_count = 0\n",
    "    discounted_product_added_count = 0\n",
    "    cart_viewed_count = 0\n",
    "    discounted_cart_viewed_count = 0\n",
    "    checkout_started_count = 0\n",
    "    discounted_checkout_started_count = 0\n",
    "    order_completed_count = 0\n",
    "    discounted_order_completed_count = 0\n",
    "\n",
    "    Path(out_interactions_filename).parents[0].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ensure determinism\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    start_time_progress = int(time.time())\n",
    "    next_timestamp = FIRST_TIMESTAMP\n",
    "    seconds_increment = int((LAST_TIMESTAMP - FIRST_TIMESTAMP) / min_interactions)\n",
    "    next_update_progress = start_time_progress + PROGRESS_MONITOR_SECONDS_UPDATE/2\n",
    "\n",
    "    average_product_price = int(products_df.price.mean())\n",
    "    print('Average product price: ${:.2f}'.format(average_product_price))\n",
    "\n",
    "    if seconds_increment <= 0: raise AssertionError(f\"Should never happen: {seconds_increment} <= 0\")\n",
    "\n",
    "    print('Minimum interactions to generate: {}'.format(min_interactions))\n",
    "    print('Starting timestamp: {} ({})'.format(next_timestamp,\n",
    "                                               time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(next_timestamp))))\n",
    "    print('Seconds increment: {}'.format(seconds_increment))\n",
    "\n",
    "    print(\"Generating interactions... (this may take a few minutes)\")\n",
    "    interactions = 0\n",
    "\n",
    "    subsets_cache = {}\n",
    "\n",
    "    user_to_product = defaultdict(set)\n",
    "\n",
    "    category_affinity_probs = np.array(CATEGORY_AFFINITY_PROBS)\n",
    "\n",
    "    print(\"Writing interactions to: {}\".format(out_interactions_filename))\n",
    "\n",
    "    with open(out_interactions_filename, 'w') as outfile:\n",
    "        f = csv.writer(outfile)\n",
    "        f.writerow([\"ITEM_ID\", \"USER_ID\", \"EVENT_TYPE\", \"TIMESTAMP\", \"DISCOUNT\"])\n",
    "\n",
    "        category_frequencies = products_df.category.value_counts()\n",
    "        category_frequencies /= sum(category_frequencies.values)\n",
    "\n",
    "        interaction_product_counts = defaultdict(int)\n",
    "\n",
    "        # Here we build up a list for each category/gender, of product\n",
    "        # affinities. The product affinity is keyed by one product,\n",
    "        # so we do not end up with exactly PRODUCT_AFFINITY_N sized\n",
    "        # cliques. They overlap a little over multiple users\n",
    "        # - that is why PRODUCT_AFFINITY_N\n",
    "        # can be a little bit lower than a desired clique size.\n",
    "        all_categories = products_df.category.unique()\n",
    "        product_affinities_bycatgender = {}\n",
    "        for category in all_categories:\n",
    "            for gender in ['M', 'F']:\n",
    "                products_cat = products_df.loc[products_df.category==category]\n",
    "                products_cat = products_cat.loc[\n",
    "                    products_cat.gender_affinity.isnull()|(products_cat.gender_affinity==gender)].id.values\n",
    "                # We ensure that all products have PRODUCT_AFFINITY_N products that lead into it\n",
    "                # and PRODUCT_AFFINITY_N products it leads to\n",
    "                affinity_matrix = sum([np.roll(np.identity(len(products_cat)), [0, i], [0, 1])\n",
    "                                       for i in range(PRODUCT_AFFINITY_N)])\n",
    "                np.random.shuffle(affinity_matrix)\n",
    "                affinity_matrix = affinity_matrix.T\n",
    "                np.random.shuffle(affinity_matrix)\n",
    "                affinity_matrix = affinity_matrix.astype(bool)  # use as boolean index\n",
    "                affinity_matrix = affinity_matrix | np.identity(len(products_cat), dtype=bool)\n",
    "\n",
    "                product_infinities = [products_cat[row] for row in affinity_matrix]\n",
    "                product_affinities_bycatgender[(category, gender)] = {\n",
    "                    products_cat[i]: products_df.loc[products_df.id.isin(product_infinities[i])]\n",
    "                    for i in range(len(products_cat))}\n",
    "\n",
    "        user_category_to_first_prod = {}\n",
    "\n",
    "        while interactions < min_interactions:\n",
    "            if (time.time() > next_update_progress):\n",
    "                rate = interactions / (time.time() - start_time_progress)\n",
    "                to_go = (min_interactions - interactions) / rate\n",
    "                print('Generated {} interactions so far (about {} seconds to go)'.format(interactions, int(to_go)))\n",
    "                next_update_progress += PROGRESS_MONITOR_SECONDS_UPDATE\n",
    "\n",
    "            # Pick a random user\n",
    "            user = users_df.loc[random.randint(0, users_df.shape[0] - 1)]\n",
    "\n",
    "            # Determine category affinity from user's persona\n",
    "            persona = user['persona']\n",
    "            # If user persona has sub-categories, we will use those sub-categories to find products for users to partake\n",
    "            # in interactions with. Otehrwise, we will use the high-level categories.\n",
    "            has_subcategories = ':' in user['persona']\n",
    "            preferred_categories_and_subcats = persona.split('_')\n",
    "            preferred_highlevel_categories = [catstring.split(':')[0] for catstring in preferred_categories_and_subcats]\n",
    "            # preferred_styles = [catstring.split(':')[1] for catstring in preferred_categories_and_subcats]\n",
    "\n",
    "            p_normalised = (category_affinity_probs * category_frequencies[preferred_highlevel_categories].values)\n",
    "            p_normalised /= p_normalised.sum()\n",
    "            p = NORMALISE_PER_PRODUCT_WEIGHT * p_normalised + (1-NORMALISE_PER_PRODUCT_WEIGHT) * category_affinity_probs\n",
    "\n",
    "            # Select category based on weighted preference of category order.\n",
    "            chosen_category_ind = np.random.choice(list(range(len(preferred_categories_and_subcats))), 1, p=p)[0]\n",
    "            category = preferred_highlevel_categories[chosen_category_ind]\n",
    "            #category_and_subcat = np.random.choice(preferred_categories_and_subcats, 1, p=p)[0]\n",
    "\n",
    "            discount_persona = user['discount_persona']\n",
    "\n",
    "            gender = user['gender']\n",
    "\n",
    "            if has_subcategories:\n",
    "                # if there is a preferred style we choose from those products with this style and category\n",
    "                # but we ignore gender.\n",
    "                # We also do not attempt to keep balance across categories.\n",
    "                style = preferred_categories_and_subcats[chosen_category_ind].split(':')[1]\n",
    "                cachekey = ('category-style', category, style)\n",
    "                prods_subset_df = subsets_cache.get(cachekey)\n",
    "\n",
    "                if prods_subset_df is None:\n",
    "                    # Select products from selected category without gender affinity or that match user's gender\n",
    "                    prods_subset_df = products_df.loc[(products_df['category']==category) &\n",
    "                                                      (products_df['style']==style)]\n",
    "                    # Update cache for quicker lookup next time\n",
    "                    subsets_cache[cachekey] = prods_subset_df\n",
    "            else:\n",
    "                # We are only going to use the machinery to keep things balanced\n",
    "                # if there is no style appointed on the user preferences.\n",
    "                # Here, in order to keep the number of products that are related to a product,\n",
    "                # we restrict the size of the set of products that are recommended to an individual\n",
    "                # user - in effect, the available subset for a particular category/gender\n",
    "                # depends on the first product selected, which is selected as per previous logic\n",
    "                # (looking at category affinities and gender)\n",
    "                usercat_key = (user['id'], category)  # has this user already selected a \"first\" product?\n",
    "                if usercat_key in user_category_to_first_prod:\n",
    "                    # If a first product is already selected, we use the product affinities for that product\n",
    "                    # To provide the list of products to select from\n",
    "                    first_prod = user_category_to_first_prod[usercat_key]\n",
    "                    prods_subset_df = product_affinities_bycatgender[(category, gender)][first_prod]\n",
    "\n",
    "                if not usercat_key in user_category_to_first_prod:\n",
    "                    # If the user has not yet selected a first product for this category\n",
    "                    # we do it by choosing between all products for gender.\n",
    "\n",
    "                    # First, check if subset data frame is already cached for category & gender\n",
    "                    cachekey = ('category-gender', category, gender)\n",
    "                    prods_subset_df = subsets_cache.get(cachekey)\n",
    "                    if prods_subset_df is None:\n",
    "                        # Select products from selected category without gender affinity or that match user's gender\n",
    "                        prods_subset_df = products_df.loc[(products_df['category'] == category) & (\n",
    "                                    (products_df['gender_affinity'] == gender) | (products_df['gender_affinity'].isnull()))]\n",
    "                        # Update cache\n",
    "                        subsets_cache[cachekey] = prods_subset_df\n",
    "\n",
    "            # Pick a random product from gender filtered subset\n",
    "            product = prods_subset_df.sample().iloc[0]\n",
    "\n",
    "            interaction_product_counts[product.id] += 1\n",
    "\n",
    "            user_to_product[user['id']].add(product['id'])\n",
    "\n",
    "            if not usercat_key in user_category_to_first_prod:\n",
    "                user_category_to_first_prod[usercat_key] = product['id']\n",
    "\n",
    "            # Decide if the product the user is interacting with is discounted\n",
    "            if discount_persona == 'discount_indifferent':\n",
    "                discounted = random.random() < DISCOUNT_PROBABILITY\n",
    "            elif discount_persona == 'all_discounts':\n",
    "                discounted = random.random() < DISCOUNT_PROBABILITY_WITH_PREFERENCE\n",
    "            elif discount_persona == 'lower_priced_products':\n",
    "                if product.price < average_product_price:\n",
    "                    discounted = random.random() < DISCOUNT_PROBABILITY_WITH_PREFERENCE\n",
    "                else:\n",
    "                    discounted = random.random() < DISCOUNT_PROBABILITY\n",
    "            else:\n",
    "                raise ValueError(f'Unable to handle discount persona: {discount_persona}')\n",
    "\n",
    "            num_interaction_sets_to_insert = 1\n",
    "            prodcnts = list(interaction_product_counts.values())\n",
    "            prodcnts_max = max(prodcnts) if len(prodcnts) > 0 else 0\n",
    "            prodcnts_min = min(prodcnts) if len(prodcnts) > 0 else 0\n",
    "            prodcnts_avg = sum(prodcnts)/len(prodcnts) if len(prodcnts) > 0 else 0\n",
    "            if interaction_product_counts[product.id] * 2 < prodcnts_max:\n",
    "                num_interaction_sets_to_insert += 1\n",
    "            if interaction_product_counts[product.id] < prodcnts_avg:\n",
    "                num_interaction_sets_to_insert += 1\n",
    "            if interaction_product_counts[product.id] == prodcnts_min:\n",
    "                num_interaction_sets_to_insert += 1\n",
    "\n",
    "            for _ in range(num_interaction_sets_to_insert):\n",
    "\n",
    "                discount_context = 'Yes' if discounted else 'No'\n",
    "\n",
    "                this_timestamp = next_timestamp + random.randint(1, seconds_increment)\n",
    "                f.writerow([product['id'],\n",
    "                            user['id'],\n",
    "                            'View',\n",
    "                            this_timestamp,\n",
    "                            discount_context])\n",
    "\n",
    "                next_timestamp += seconds_increment\n",
    "                product_viewed_count += 1\n",
    "                interactions += 1\n",
    "\n",
    "                if discounted:\n",
    "                    discounted_product_viewed_count += 1\n",
    "\n",
    "                if product_added_count < int(product_viewed_count * product_added_percent):\n",
    "                    this_timestamp += random.randint(1, int(seconds_increment / 2))\n",
    "                    f.writerow([product['id'],\n",
    "                                user['id'],\n",
    "                                'AddToCart',\n",
    "                                this_timestamp,\n",
    "                                discount_context])\n",
    "                    interactions += 1\n",
    "                    product_added_count += 1\n",
    "\n",
    "                    if discounted:\n",
    "                        discounted_product_added_count += 1\n",
    "\n",
    "                if cart_viewed_count < int(product_viewed_count * cart_viewed_percent):\n",
    "                    this_timestamp += random.randint(1, int(seconds_increment / 2))\n",
    "                    f.writerow([product['id'],\n",
    "                                user['id'],\n",
    "                                'ViewCart',\n",
    "                                this_timestamp,\n",
    "                                discount_context])\n",
    "                    interactions += 1\n",
    "                    cart_viewed_count += 1\n",
    "                    if discounted:\n",
    "                        discounted_cart_viewed_count += 1\n",
    "\n",
    "                if checkout_started_count < int(product_viewed_count * checkout_started_percent):\n",
    "                    this_timestamp += random.randint(1, int(seconds_increment / 2))\n",
    "                    f.writerow([product['id'],\n",
    "                                user['id'],\n",
    "                                'StartCheckout',\n",
    "                                this_timestamp,\n",
    "                                discount_context])\n",
    "                    interactions += 1\n",
    "                    checkout_started_count += 1\n",
    "                    if discounted:\n",
    "                           discounted_checkout_started_count += 1\n",
    "\n",
    "                if order_completed_count < int(product_viewed_count * order_completed_percent):\n",
    "                    this_timestamp += random.randint(1, int(seconds_increment / 2))\n",
    "                    f.writerow([product['id'],\n",
    "                                user['id'],\n",
    "                                'Purchase',\n",
    "                                this_timestamp,\n",
    "                                discount_context])\n",
    "                    interactions += 1\n",
    "                    order_completed_count += 1\n",
    "                    if discounted:\n",
    "                        discounted_order_completed_count += 1\n",
    "\n",
    "    print(\"Interactions generation done.\")\n",
    "    print(f\"Total interactions: {interactions}\")\n",
    "    print(f\"Total product viewed: {product_viewed_count} ({discounted_product_viewed_count})\")\n",
    "    print(f\"Total product added: {product_added_count} ({discounted_product_added_count})\")\n",
    "    print(f\"Total cart viewed: {cart_viewed_count} ({discounted_cart_viewed_count})\")\n",
    "    print(f\"Total checkout started: {checkout_started_count} ({discounted_checkout_started_count})\")\n",
    "    print(f\"Total order completed: {order_completed_count} ({discounted_order_completed_count})\")\n",
    "\n",
    "    globals().update(locals())   # This can be used for inspecting in console after script ran or if run with ipython.\n",
    "    print('Generation script finished')\n",
    "\n",
    "# Run\n",
    "# if __name__ == '__main__':\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "users_df, products_df = generate_user_items(out_users_filename, out_items_filename, IN_USERS_FILENAMES, IN_PRODUCTS_FILENAME)\n",
    "generate_interactions(out_interactions_filename, users_df, products_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547aafe",
   "metadata": {},
   "source": [
    "## Personalize Workshop Cleanup - Lab 9 (optional)\n",
    "This notebook will walk through deleting all of the resources created by the Personalization Labs in this workshop. You should only need to perform these steps if you have deployed the Retail Demo Store in your own AWS account and want to deprovision the Personalize resources. If you are participating in an AWS-led workshop, this process is likely not necessary.\n",
    "\n",
    "Resources have to deleted in a specific sequence to avoid dependency errors. In order, we will delete recommenders and campaigns, solutions, event trackers, filters, datasets, and the dataset group. In addition, we need to make sure that each resource type is fully deleted before moving on to the next resource type. We'll also delete the schemas for our datasets and reset the SSM parameter values that are used by the Recommendations service and Web UI for personalization features.\n",
    "\n",
    "We will be leveraging a utility module written on python that provides an orderly delete process for deleting all resources in each dataset group.\n",
    "\n",
    "### Import depedencies and adjust path\n",
    "The following code cell ensures that we are working with the latest version of the boto3 library and that the path is updated so we can load the delete_dataset_groups module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce961ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./lib/python3.7/site-packages (22.2.2)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.27.75-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.42\n",
      "    Uninstalling botocore-1.27.42:\n",
      "      Successfully uninstalled botocore-1.27.42\n",
      "Successfully installed botocore-1.27.75\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore\n",
    "sys.path.insert(0, os.path.abspath('../../src/aws-lambda/personalize-pre-create-resources'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658aa6e9",
   "metadata": {},
   "source": [
    "### Reset SSM parameters\n",
    "Before deleting resources, we will reset the SSM parameter values so that the Recommendations service will no longer attempt to make requests to deleted resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "648e3be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSM parameters have been reset\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "param_names = [\n",
    "    '/retaildemostore/personalize/event-tracker-id'\n",
    "    '/retaildemostore/personalize/filters/filter-purchased-arn',\n",
    "    '/retaildemostore/personalize/filters/filter-cstore-arn',\n",
    "    '/retaildemostore/personalize/filters/filter-purchased-and-cstore-arn',\n",
    "    '/retaildemostore/personalize/recommended-for-you-arn',\n",
    "    '/retaildemostore/personalize/popular-items-arn',\n",
    "    '/retaildemostore/personalize/related-items-arn',\n",
    "    '/retaildemostore/personalize/personalized-ranking-arn',\n",
    "    '/retaildemostore/personalize/personalized-offers-arn'\n",
    "]\n",
    "\n",
    "ssm = boto3.client('ssm')\n",
    "\n",
    "for param_name in param_names:\n",
    "    ssm.put_parameter(\n",
    "        Name=param_name,\n",
    "        Value='NONE',\n",
    "        Type='String',\n",
    "        Overwrite=True\n",
    "    )\n",
    "    \n",
    "print('SSM parameters have been reset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a4922",
   "metadata": {},
   "source": [
    "### Prepare and delete dataset groups\n",
    "We are now ready to delete the active workshop dataset groups created by the labs involving Amazon Personalize. The delete_dataset_groups function will delete all dependent resources within the dataset groups starting with recommenders and campaigns, then solutions and datasets, filters and event trackers, and ending with schemas and the dataset groups themselves.\n",
    "\n",
    "#### Identify active workshop dataset groups\n",
    "Let's start by checking the active dataset groups in the current account and check which of the two potential dataset groups are active. This will tell us which dataset group(s) need to be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d50fdd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active workshop dataset groups that need to be deleted: ['retaildemostore-products']\n"
     ]
    }
   ],
   "source": [
    "personalize = boto3.client('personalize')\n",
    "\n",
    "# Dataset group names that may have been created by workshop labs.\n",
    "possible_dataset_groups = [\n",
    "    'retaildemostore-products',\n",
    "    'retaildemostore-offers'\n",
    "]\n",
    "\n",
    "# Actual workshop dataset groups that exist. The logic below will add to this list.\n",
    "active_dataset_groups = []\n",
    "\n",
    "# Check dataset groups to see which workshop dataset groups were actually created.\n",
    "paginator = personalize.get_paginator('list_dataset_groups')\n",
    "for paginate_result in paginator.paginate():\n",
    "    for dataset_group in paginate_result[\"datasetGroups\"]:\n",
    "        if dataset_group['name'] in possible_dataset_groups:\n",
    "            active_dataset_groups.append(dataset_group['name'])\n",
    "\n",
    "print(f'Active workshop dataset groups that need to be deleted: {active_dataset_groups}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9101f",
   "metadata": {},
   "source": [
    "#### Prepare logging output\n",
    "Next we will import the delete_dataset_groups (https://github.com/aws-samples/retail-demo-store/blob/29f4e384e4ffb3a4a2966b6993120a002290aea0/src/aws-lambda/personalize-pre-create-resources/delete_dataset_groups.py) module and update the logging in the module so we can see its output here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50af46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied this file from demo git to my local machine to /Users/Andrew/Documents/python/env\n",
    "import delete_dataset_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28cc9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "delete_dataset_groups.logger.setLevel(logging.INFO)\n",
    "delete_dataset_groups.logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ea846",
   "metadata": {},
   "source": [
    "#### Delete dataset groups\n",
    "Now we can delete the active workshop dataset groups. This can take several minutes depending on the number of dataset groups and resources within each dataset group. The function below will log its progress until finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63360bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset Group ARN: arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Group ARN: arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products\n",
      "Dataset Group ARN: arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting recommender arn:aws:personalize:us-east-1:016882278218:recommender/retaildemostore-recommended-for-you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting recommender arn:aws:personalize:us-east-1:016882278218:recommender/retaildemostore-recommended-for-you\n",
      "Deleting recommender arn:aws:personalize:us-east-1:016882278218:recommender/retaildemostore-recommended-for-you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting recommender arn:aws:personalize:us-east-1:016882278218:recommender/retaildemostore-popular-items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting recommender arn:aws:personalize:us-east-1:016882278218:recommender/retaildemostore-popular-items\n",
      "Deleting recommender arn:aws:personalize:us-east-1:016882278218:recommender/retaildemostore-popular-items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting campaign: arn:aws:personalize:us-east-1:016882278218:campaign/retaildemostore-related-items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting campaign: arn:aws:personalize:us-east-1:016882278218:campaign/retaildemostore-related-items\n",
      "Deleting campaign: arn:aws:personalize:us-east-1:016882278218:campaign/retaildemostore-related-items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting campaign: arn:aws:personalize:us-east-1:016882278218:campaign/retaildemostore-personalized-ranking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting campaign: arn:aws:personalize:us-east-1:016882278218:campaign/retaildemostore-personalized-ranking\n",
      "Deleting campaign: arn:aws:personalize:us-east-1:016882278218:campaign/retaildemostore-personalized-ranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 recommender(s) to be deleted\n",
      "Waiting for 2 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 recommender(s) to be deleted\n",
      "Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 recommender(s) to be deleted\n",
      "Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 recommender(s) to be deleted\n",
      "Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 recommender(s) to be deleted\n",
      "Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 recommender(s) to be deleted\n",
      "Waiting for 1 recommender(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All recommenders have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All recommenders have been deleted or none exist for dataset group\n",
      "All recommenders have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 campaign(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 campaign(s) to be deleted\n",
      "Waiting for 1 campaign(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All campaigns have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All campaigns have been deleted or none exist for dataset group\n",
      "All campaigns have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-related-items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-related-items\n",
      "Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-related-items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-personalized-ranking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-personalized-ranking\n",
      "Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-personalized-ranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-item-attribute-affinity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-item-attribute-affinity\n",
      "Deleting solution: arn:aws:personalize:us-east-1:016882278218:solution/retaildemostore-item-attribute-affinity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 3 solution(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 3 solution(s) to be deleted\n",
      "Waiting for 3 solution(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 3 solution(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 3 solution(s) to be deleted\n",
      "Waiting for 3 solution(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 2 solution(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 2 solution(s) to be deleted\n",
      "Waiting for 2 solution(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 solution(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 solution(s) to be deleted\n",
      "Waiting for 1 solution(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All solutions have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All solutions have been deleted or none exist for dataset group\n",
      "All solutions have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting event tracker arn:aws:personalize:us-east-1:016882278218:event-tracker/b7f4b38e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting event tracker arn:aws:personalize:us-east-1:016882278218:event-tracker/b7f4b38e\n",
      "Deleting event tracker arn:aws:personalize:us-east-1:016882278218:event-tracker/b7f4b38e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 event tracker(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 event tracker(s) to be deleted\n",
      "Waiting for 1 event tracker(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All event trackers have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All event trackers have been deleted or none exist for dataset group\n",
      "All event trackers have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All filters have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All filters have been deleted or none exist for dataset group\n",
      "All filters have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/USERS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/USERS\n",
      "Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/USERS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/INTERACTIONS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/INTERACTIONS\n",
      "Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/INTERACTIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/ITEMS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/ITEMS\n",
      "Deleting dataset arn:aws:personalize:us-east-1:016882278218:dataset/retaildemostore-products/ITEMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 3 dataset(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 3 dataset(s) to be deleted\n",
      "Waiting for 3 dataset(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 dataset(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 dataset(s) to be deleted\n",
      "Waiting for 1 dataset(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for 1 dataset(s) to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for 1 dataset(s) to be deleted\n",
      "Waiting for 1 dataset(s) to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All datasets have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets have been deleted or none exist for dataset group\n",
      "All datasets have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-users\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-users\n",
      "Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-interactions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-interactions\n",
      "Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-interactions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-items\n",
      "Deleting schema arn:aws:personalize:us-east-1:016882278218:schema/retaildemostore-products-items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:All schemas used exclusively by datasets have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All schemas used exclusively by datasets have been deleted or none exist for dataset group\n",
      "All schemas used exclusively by datasets have been deleted or none exist for dataset group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deleting dataset group arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting dataset group arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products\n",
      "Deleting dataset group arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for dataset group to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for dataset group to be deleted\n",
      "Waiting for dataset group to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for dataset group to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for dataset group to be deleted\n",
      "Waiting for dataset group to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for dataset group to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for dataset group to be deleted\n",
      "Waiting for dataset group to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for dataset group to be deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for dataset group to be deleted\n",
      "Waiting for dataset group to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset group arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products has been fully deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset group arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products has been fully deleted\n",
      "Dataset group arn:aws:personalize:us-east-1:016882278218:dataset-group/retaildemostore-products has been fully deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset group retaildemostore-products fully deleted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset group retaildemostore-products fully deleted\n",
      "Dataset group retaildemostore-products fully deleted\n",
      "CPU times: user 693 ms, sys: 237 ms, total: 929 ms\n",
      "Wall time: 15min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "delete_dataset_groups.delete_dataset_groups(\n",
    "    dataset_group_names = active_dataset_groups, \n",
    "    wait_for_resources = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923b987",
   "metadata": {},
   "source": [
    "### Cleanup Complete\n",
    "All resources created by the Personalize workshop(s) have been deleted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
